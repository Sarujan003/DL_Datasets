# -*- coding: utf-8 -*-
"""DL_MINIPROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bK5lAo6FABWDqp0nmfsSX0irmY7VvkeF

### STEP 1: SETUP & IMPORTS
"""

import os
import pandas as pd
import numpy as np
import spacy
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras import layers, models
from tensorflow.keras.utils import Sequence
import pickle
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, BatchNormalization

!git clone https://github.com/Sarujan003/DL_Datasets.git

# Load captions
caption_path = "DL_Datasets/captions.txt"
captions_df = pd.read_csv(caption_path)

image_dir = "DL_Datasets/compressed_images"

"""### STEP 2: LOAD CAPTIONS"""

# Load SpaCy model
import spacy.cli
spacy.cli.download("en_core_web_sm")
nlp = spacy.load("en_core_web_sm")

# Load captions
caption_path = "DL_Datasets/captions.txt"
captions_df = pd.read_csv(caption_path)

# Extract nouns and verbs for each image
keywords_dict = {}
for image, group in tqdm(captions_df.groupby("image")):
    keywords = set()
    for caption in group['caption']:
        doc = nlp(caption)
        for token in doc:
            if token.pos_ in ["NOUN", "VERB"]:
                keywords.add(token.lemma_.lower())
    keywords_dict[image] = list(keywords)

# Create dataframe
df = pd.DataFrame(list(keywords_dict.items()), columns=["image_name", "keywords"])
df.to_csv("image_keywords.csv", index=False)

"""### STEP 3: MULTI-LABEL BINARIZER"""

mlb = MultiLabelBinarizer()
df['keywords'] = df['keywords'].apply(lambda x: list(set(x)))  # Ensure uniqueness
Y = mlb.fit_transform(df['keywords'])

# Save MultiLabelBinarizer
with open("mlb.pkl", "wb") as f:
    pickle.dump(mlb, f)

"""### STEP 4: CUSTOM DATA GENERATOR"""

class ImageKeywordGenerator(Sequence):
    def __init__(self, df, labels, image_dir, mlb, batch_size=32, img_size=(224, 224), shuffle=True):
        self.df = df.reset_index(drop=True)
        self.labels = labels
        self.image_dir = image_dir
        self.batch_size = batch_size
        self.img_size = img_size
        self.shuffle = shuffle
        self.mlb = mlb
        self.indexes = np.arange(len(self.df))
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_df = self.df.iloc[batch_indexes]

        X, Y = [], []
        for idx in batch_indexes:
            row = self.df.iloc[idx]
            img_path = os.path.join(self.image_dir, row['image_name'])
            try:
                img = load_img(img_path, target_size=self.img_size)
                img_array = img_to_array(img) / 255.0
                X.append(img_array)
                Y.append(self.labels[idx])
            except FileNotFoundError:
                continue

        return np.array(X), np.array(Y)

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indexes)

"""### STEP 5: SPLIT DATA"""

image_dir = "DL_Datasets/compressed_images"
train_df, val_df, train_labels, val_labels = train_test_split(df, Y, test_size=0.2, random_state=42)

"""### STEP 6: CNN HYPERPARAMETER TUNING"""

# Define hyperparameter configurations to test for CNN
cnn_configs = [
    # Config 1: Simple model with fewer filters
    {
        'filters': [16, 32, 64],
        'kernel_size': (3, 3),
        'dense_units': 64,
        'dropout_rate': 0.0,
        'batch_size': 32,
        'learning_rate': 0.001,
        'use_batch_norm': False
    },
    # Config 2: More filters, with dropout
    {
        'filters': [32, 64, 128],
        'kernel_size': (3, 3),
        'dense_units': 128,
        'dropout_rate': 0.3,
        'batch_size': 32,
        'learning_rate': 0.001,
        'use_batch_norm': False
    },
    # Config 3: Complex model with batch normalization
    {
        'filters': [64, 128, 256],
        'kernel_size': (3, 3),
        'dense_units': 256,
        'dropout_rate': 0.5,
        'batch_size': 16,
        'learning_rate': 0.0005,
        'use_batch_norm': True
    }
]

# Function to create CNN model
def create_cnn_model(config, input_shape=(224, 224, 3), num_classes=len(mlb.classes_)):
    model = models.Sequential()

    # First Conv layer
    model.add(layers.Conv2D(config['filters'][0], config['kernel_size'], activation='relu', input_shape=input_shape))
    model.add(layers.MaxPooling2D(2, 2))
    if config['use_batch_norm']:
        model.add(layers.BatchNormalization())
    if config['dropout_rate'] > 0:
        model.add(layers.Dropout(config['dropout_rate']))

    # Second Conv layer
    model.add(layers.Conv2D(config['filters'][1], config['kernel_size'], activation='relu'))
    model.add(layers.MaxPooling2D(2, 2))
    if config['use_batch_norm']:
        model.add(layers.BatchNormalization())
    if config['dropout_rate'] > 0:
        model.add(layers.Dropout(config['dropout_rate']))

    # Third Conv layer
    model.add(layers.Conv2D(config['filters'][2], config['kernel_size'], activation='relu'))
    model.add(layers.MaxPooling2D(2, 2))
    if config['use_batch_norm']:
        model.add(layers.BatchNormalization())
    if config['dropout_rate'] > 0:
        model.add(layers.Dropout(config['dropout_rate']))

    # Flatten and Dense layers
    model.add(layers.Flatten())
    model.add(layers.Dense(config['dense_units'], activation='relu'))
    if config['dropout_rate'] > 0:
        model.add(layers.Dropout(config['dropout_rate']))
    model.add(layers.Dense(num_classes, activation='sigmoid'))  # Multi-label output

    from tensorflow.keras.optimizers import Adam
    model.compile(
        optimizer=Adam(learning_rate=config['learning_rate']),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

# Train and evaluate each CNN configuration for 3 epochs
cnn_results = []

for i, config in enumerate(cnn_configs):
    print(f"\n=== Training CNN Configuration {i+1} ===")
    print(f"Configuration: {config}")

    # Create generators with the specific batch size
    train_gen = ImageKeywordGenerator(train_df, train_labels, image_dir, mlb, batch_size=config['batch_size'])
    val_gen = ImageKeywordGenerator(val_df, val_labels, image_dir, mlb, batch_size=config['batch_size'], shuffle=False)

    # Create and train model
    model = create_cnn_model(config)
    history = model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=3,
        verbose=1
    )

    # Record results
    val_loss = history.history['val_loss'][-1]
    val_acc = history.history['val_accuracy'][-1]
    cnn_results.append({
        'config_idx': i,
        'config': config,
        'val_loss': val_loss,
        'val_accuracy': val_acc
    })

    # Save model checkpoint
    model.save(f"cnn_model_config_{i}.h5")

    print(f"Configuration {i+1} validation loss: {val_loss:.4f}, validation accuracy: {val_acc:.4f}")

# Find the best CNN configuration
best_cnn_config_idx = min(range(len(cnn_results)), key=lambda i: cnn_results[i]['val_loss'])
best_cnn_config = cnn_configs[best_cnn_config_idx]

print("\n=== Best CNN Configuration ===")
print(f"Configuration {best_cnn_config_idx + 1}: {best_cnn_config}")
print(f"Validation Loss: {cnn_results[best_cnn_config_idx]['val_loss']:.4f}")
print(f"Validation Accuracy: {cnn_results[best_cnn_config_idx]['val_accuracy']:.4f}")

"""### STEP 7: TRAIN BEST CNN MODEL FOR 15 EPOCHS"""

print("\n=== Training Best CNN Model for 15 Epochs ===")

# Create generators with the best batch size
train_gen = ImageKeywordGenerator(train_df, train_labels, image_dir, mlb, batch_size=best_cnn_config['batch_size'])
val_gen = ImageKeywordGenerator(val_df, val_labels, image_dir, mlb, batch_size=best_cnn_config['batch_size'], shuffle=False)

# Create the best model
best_cnn_model = create_cnn_model(best_cnn_config)

# Define callbacks
cnn_checkpoint = ModelCheckpoint(
    filepath='best_keyword_prediction_model.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

# Train for 15 epochs
best_cnn_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=15,
    callbacks=[cnn_checkpoint, early_stopping],
    verbose=1
)

# Save the final model
best_cnn_model.save("final_keyword_prediction_model.h5")

import matplotlib.pyplot as plt

epochs = list(range(1, 11))

train_accuracy = [0.0030, 0.0088, 0.0227, 0.0228, 0.0198, 0.0222, 0.0196, 0.0249, 0.0287, 0.0292]
val_accuracy   = [0.0012, 0.0340, 0.0185, 0.0216, 0.0136, 0.0161, 0.0241, 0.0198, 0.0210, 0.0253]

train_loss = [0.0922, 0.0134, 0.0133, 0.0131, 0.0129, 0.0127, 0.0125, 0.0122, 0.0119, 0.0115]
val_loss   = [0.0135, 0.0135, 0.0132, 0.0131, 0.0131, 0.0130, 0.0130, 0.0131, 0.0132, 0.0132]

# Plotting Accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_accuracy, label='Training Accuracy', marker='o')
plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Plotting Loss
plt.subplot(1, 2, 2)
plt.plot(epochs, train_loss, label='Training Loss', marker='o')
plt.plot(epochs, val_loss, label='Validation Loss', marker='o')
plt.title('Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""### STEP 8: PREPARE DATA FOR CAPTION GENERATION"""

# Load the keywords
keywords_df = pd.read_csv("image_keywords.csv")

# Load the captions
captions_df = pd.read_csv("DL_Datasets/captions.txt")

# Group captions by image
captions_grouped = captions_df.groupby('image')['caption'].apply(list).reset_index()

# Merge with keywords on image_name = image
merged_df = pd.merge(keywords_df, captions_grouped, left_on='image_name', right_on='image', how='inner')

# Drop the redundant 'image' column
merged_df.drop(columns=['image'], inplace=True)

# Expand the list of captions into separate columns
max_captions = merged_df['caption'].apply(len).max()  # max number of captions per image

for i in range(max_captions):
    merged_df[f'caption_{i+1}'] = merged_df['caption'].apply(lambda x: x[i] if i < len(x) else "")

# Drop the original 'caption' list column
merged_df.drop(columns=['caption'], inplace=True)

# Save to CSV
merged_df.to_csv("merged_image_keywords_captions.csv", index=False)

"""### STEP 9: PREPARE DATA FOR LSTM"""

df = pd.read_csv("merged_image_keywords_captions.csv")

# Combine all keyword â†’ caption pairs
data_pairs = []

for _, row in df.iterrows():
    keywords = row["keywords"]
    keywords = keywords.strip("[]").replace("'", "").split(", ")
    keyword_string = " ".join(keywords)
    for i in range(1, 6):  # 5 captions: caption_1 to caption_5
        caption_col = f"caption_{i}"
        if pd.notna(row[caption_col]) and row[caption_col].strip():
            caption = row[caption_col].strip().lower()
            data_pairs.append((keyword_string, caption))

print(f"Total samples: {len(data_pairs)}")

# Separate inputs and targets
inputs, targets = zip(*data_pairs)

# Add start/end tokens to targets BEFORE tokenizing
targets = [f"<start> {t} <end>" for t in targets]

# Tokenizers
input_tokenizer = Tokenizer(oov_token="<OOV>")
input_tokenizer.fit_on_texts(inputs)
input_sequences = input_tokenizer.texts_to_sequences(inputs)
input_maxlen = max(len(seq) for seq in input_sequences)

output_tokenizer = Tokenizer(oov_token="<OOV>")
output_tokenizer.fit_on_texts(targets)
target_sequences = output_tokenizer.texts_to_sequences(targets)
target_maxlen = max(len(seq) for seq in target_sequences)

# Pad sequences
encoder_input = pad_sequences(input_sequences, maxlen=input_maxlen, padding='post')
decoder_input = pad_sequences([seq[:-1] for seq in target_sequences], maxlen=target_maxlen-1, padding='post')
decoder_target = pad_sequences([seq[1:] for seq in target_sequences], maxlen=target_maxlen-1, padding='post')

# Vocabulary sizes
input_vocab_size = len(input_tokenizer.word_index) + 1
output_vocab_size = len(output_tokenizer.word_index) + 1

# Split data for LSTM model
train_idx, val_idx = train_test_split(range(len(encoder_input)), test_size=0.1, random_state=42)

encoder_input_train = encoder_input[train_idx]
decoder_input_train = decoder_input[train_idx]
decoder_target_train = decoder_target[train_idx]

encoder_input_val = encoder_input[val_idx]
decoder_input_val = decoder_input[val_idx]
decoder_target_val = decoder_target[val_idx]

"""### STEP 10: LSTM HYPERPARAMETER TUNING"""

# Define hyperparameter configurations to test for LSTM
lstm_configs = [
    # Config 1: Smaller model
    {
        'embedding_dim': 64,
        'lstm_units': 128,
        'dropout_rate': 0.0,
        'learning_rate': 0.001,
        'batch_size': 64
    },
    # Config 2: Medium model with dropout
    {
        'embedding_dim': 128,
        'lstm_units': 256,
        'dropout_rate': 0.3,
        'learning_rate': 0.001,
        'batch_size': 64
    },
    # Config 3: Larger model with more dropout and smaller batch size
    {
        'embedding_dim': 256,
        'lstm_units': 512,
        'dropout_rate': 0.5,
        'learning_rate': 0.0005,
        'batch_size': 32
    }
]

# Function to create LSTM model
def create_lstm_model(config, input_maxlen, target_maxlen, input_vocab_size, output_vocab_size):
    # Encoder
    encoder_inputs = Input(shape=(input_maxlen,))
    encoder_emb = Embedding(input_vocab_size, config['embedding_dim'])(encoder_inputs)

    # Add dropout if specified
    if config['dropout_rate'] > 0:
        encoder_emb = Dropout(config['dropout_rate'])(encoder_emb)

    _, state_h, state_c = LSTM(config['lstm_units'], return_state=True)(encoder_emb)
    encoder_states = [state_h, state_c]

    # Decoder
    decoder_inputs = Input(shape=(target_maxlen-1,))
    decoder_emb = Embedding(output_vocab_size, config['embedding_dim'])(decoder_inputs)

    # Add dropout if specified
    if config['dropout_rate'] > 0:
        decoder_emb = Dropout(config['dropout_rate'])(decoder_emb)

    decoder_lstm = LSTM(config['lstm_units'], return_sequences=True)(decoder_emb, initial_state=encoder_states)

    # Add dropout if specified
    if config['dropout_rate'] > 0:
        decoder_lstm = Dropout(config['dropout_rate'])(decoder_lstm)

    decoder_outputs = Dense(output_vocab_size, activation='softmax')(decoder_lstm)

    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

    from tensorflow.keras.optimizers import Adam
    model.compile(
        optimizer=Adam(learning_rate=config['learning_rate']),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Train and evaluate each LSTM configuration for 3 epochs
lstm_results = []

for i, config in enumerate(lstm_configs):
    print(f"\n=== Training LSTM Configuration {i+1} ===")
    print(f"Configuration: {config}")

    # Create and train model
    model = create_lstm_model(
        config,
        input_maxlen,
        target_maxlen,
        input_vocab_size,
        output_vocab_size
    )

    history = model.fit(
        [encoder_input_train, decoder_input_train],
        np.expand_dims(decoder_target_train, -1),
        batch_size=config['batch_size'],
        epochs=3,
        validation_data=([encoder_input_val, decoder_input_val], np.expand_dims(decoder_target_val, -1)),
        verbose=1
    )

    # Record results
    val_loss = history.history['val_loss'][-1]
    val_acc = history.history['val_accuracy'][-1]
    lstm_results.append({
        'config_idx': i,
        'config': config,
        'val_loss': val_loss,
        'val_accuracy': val_acc
    })

    # Save model checkpoint
    model.save(f"lstm_model_config_{i}.h5")

    print(f"Configuration {i+1} validation loss: {val_loss:.4f}, validation accuracy: {val_acc:.4f}")

# Find the best LSTM configuration
best_lstm_config_idx = min(range(len(lstm_results)), key=lambda i: lstm_results[i]['val_loss'])
best_lstm_config = lstm_configs[best_lstm_config_idx]

print("\n=== Best LSTM Configuration ===")
print(f"Configuration {best_lstm_config_idx + 1}: {best_lstm_config}")
print(f"Validation Loss: {lstm_results[best_lstm_config_idx]['val_loss']:.4f}")
print(f"Validation Accuracy: {lstm_results[best_lstm_config_idx]['val_accuracy']:.4f}")

"""### STEP 11: TRAIN BEST LSTM MODEL FOR 15 EPOCHS"""

print("\n=== Training Best LSTM Model for 15 Epochs ===")

# Create the best model
best_lstm_model = create_lstm_model(
    best_lstm_config,
    input_maxlen,
    target_maxlen,
    input_vocab_size,
    output_vocab_size
)

# Define callbacks
lstm_checkpoint = ModelCheckpoint(
    filepath='best_caption_generator_lstm.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

early_stopping_lstm = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

# Train for 15 epochs
best_lstm_model.fit(
    [encoder_input_train, decoder_input_train],
    np.expand_dims(decoder_target_train, -1),
    batch_size=best_lstm_config['batch_size'],
    epochs=15,
    validation_data=([encoder_input_val, decoder_input_val], np.expand_dims(decoder_target_val, -1)),
    callbacks=[lstm_checkpoint, early_stopping_lstm],
    verbose=1
)

# Save the final model
best_lstm_model.save("final_caption_generator_lstm.h5")

import matplotlib.pyplot as plt

# Training and validation metrics per epoch
epochs = list(range(1, 16))

train_accuracy = [0.7342, 0.7796, 0.7895, 0.7961, 0.8039, 0.8101, 0.8148, 0.8184, 0.8212, 0.8229, 0.8254, 0.8289, 0.8297, 0.8320, 0.8345]
val_accuracy   = [0.7742, 0.7902, 0.7953, 0.8046, 0.8112, 0.8149, 0.8181, 0.8213, 0.8232, 0.8242, 0.8261, 0.8270, 0.8282, 0.8294, 0.8307]

train_loss = [1.9893, 1.2451, 1.1362, 1.0622, 0.9903, 0.9329, 0.8884, 0.8533, 0.8219, 0.7997, 0.7760, 0.7500, 0.7358, 0.7173, 0.7000]
val_loss   = [1.2809, 1.1537, 1.0882, 1.0170, 0.9648, 0.9313, 0.9045, 0.8839, 0.8701, 0.8586, 0.8524, 0.8426, 0.8346, 0.8304, 0.8264]

# Plot Accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_accuracy, label='Training Accuracy', marker='o')
plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(epochs, train_loss, label='Training Loss', marker='o')
plt.plot(epochs, val_loss, label='Validation Loss', marker='o')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""### STEP 12: EVALUATE AND GENERATE CAPTIONS"""

# Load the best models
best_cnn_model = load_model("best_keyword_prediction_model.h5")
best_lstm_model = load_model("best_caption_generator_lstm.h5")

# Load the MultiLabelBinarizer
with open("mlb.pkl", "rb") as f:
    mlb = pickle.load(f)

# Function to convert a list of keywords to a string and then tokenize
def preprocess_keywords(keywords_list):
    keyword_str = " ".join(keywords_list).lower()
    sequence = input_tokenizer.texts_to_sequences([keyword_str])
    return pad_sequences(sequence, maxlen=input_maxlen, padding='post')

def generate_caption(keywords_list):
    # Convert keyword list to padded sequence
    encoder_seq = preprocess_keywords(keywords_list)

    # Use first token from training data as <start>
    start_token_id = decoder_input[0, 0]

    # Initialize decoder input
    target_seq = np.zeros((1, target_maxlen - 1))
    target_seq[0, 0] = start_token_id

    decoded_sentence = []

    for i in range(1, target_maxlen - 1):
        preds = best_lstm_model.predict([encoder_seq, target_seq], verbose=0)
        sampled_token_index = np.argmax(preds[0, i - 1, :])
        sampled_word = output_tokenizer.index_word.get(sampled_token_index, "")

        if sampled_word == "<end>" or sampled_word == "":
            break

        decoded_sentence.append(sampled_word)
        target_seq[0, i] = sampled_token_index

    return " ".join(decoded_sentence)

# Create validation generator to test models
val_gen = ImageKeywordGenerator(val_df, val_labels, image_dir, mlb, batch_size=32, shuffle=False)

captions = []

def generate_sentence_for_image(index_):
  # Test with different validation samples
  sample_img_batch, sample_label_batch = val_gen[index_]

  if len(sample_img_batch) > 0:
      # Pick one sample (e.g., index 0)
      sample_image = sample_img_batch[0]
      sample_label = sample_label_batch[0]

      # Predict keywords from CNN
      pred = best_cnn_model.predict(np.expand_dims(sample_image, axis=0))
      predicted_keywords = mlb.inverse_transform(pred > 0.4)

      # Take the first tuple and convert to list
      keywords_list = list(predicted_keywords[0])

      # Generate caption from predicted keywords
      caption = generate_caption(keywords_list)
      captions.append(caption)

      # Show image
      plt.figure(figsize=(6, 6))
      plt.imshow(sample_image)
      plt.axis("off")
      plt.figtext(0.5, 0.01, f"Caption: {caption}", wrap=True, horizontalalignment='center')
      plt.show()

      print(f"Sample {index_}")
      print("Generated Caption:", caption)
      print("-" * 50)

generate_sentence_for_image(0)

generate_sentence_for_image(10)

# Test with different validation samples
for sample_idx in range(14,17):
    sample_img_batch, sample_label_batch = val_gen[sample_idx]

    if len(sample_img_batch) > 0:
        # Pick one sample (e.g., index 0)
        sample_image = sample_img_batch[0]
        sample_label = sample_label_batch[0]

        # Predict keywords from CNN
        pred = best_cnn_model.predict(np.expand_dims(sample_image, axis=0))
        predicted_keywords = mlb.inverse_transform(pred > 0.1)

        # Take the first tuple and convert to list
        keywords_list = list(predicted_keywords[0])

        # Generate caption from predicted keywords
        caption = generate_caption(keywords_list)
        captions.append(caption)

        # Show image
        plt.figure(figsize=(6, 6))
        plt.imshow(sample_image)
        plt.axis("off")
        plt.figtext(0.5, 0.01, f"Caption: {caption}", wrap=True, horizontalalignment='center')
        plt.show()

        print(f"Sample {sample_idx+1}")
        print("Generated Caption:", caption)
        print("-" * 50)

# Test with different validation samples
for sample_idx in range(21,30):
    sample_img_batch, sample_label_batch = val_gen[sample_idx]

    if len(sample_img_batch) > 0:
        # Pick one sample (e.g., index 0)
        sample_image = sample_img_batch[0]
        sample_label = sample_label_batch[0]

        # Predict keywords from CNN
        pred = best_cnn_model.predict(np.expand_dims(sample_image, axis=0))
        predicted_keywords = mlb.inverse_transform(pred > 0.1)

        # Take the first tuple and convert to list
        keywords_list = list(predicted_keywords[0])

        # Generate caption from predicted keywords
        caption = generate_caption(keywords_list)
        captions.append(caption)

        # Show image
        plt.figure(figsize=(6, 6))
        plt.imshow(sample_image)
        plt.axis("off")
        plt.figtext(0.5, 0.01, f"Caption: {caption}", wrap=True, horizontalalignment='center')
        plt.show()

        print(f"Sample {sample_idx+1}")
        print("Generated Caption:", caption)
        print("-" * 50)

import pandas as pd
import matplotlib.pyplot as plt
import os
from PIL import Image

# Load the captions file
caption_path = "DL_Datasets/captions.txt"
image_dir = "DL_Datasets/compressed_images"  # Directory containing the images

# List of target images
target_images = [
    "3581818450_546c89ca38.jpg",
    "3208987435_780ae35ef0.jpg",
    "3139895886_5a6d495b13.jpg",
    "1354318519_2f9baed754.jpg",
    "3381788544_2c50e139dd.jpg",
    "756004341_1a816df714.jpg"
]

# Dictionary to store captions for each target image
image_captions = {img: [] for img in target_images}

# Open and read the file manually to avoid CSV parsing issues
with open(caption_path, 'r') as file:
    # Skip header line
    next(file)
    for line in file:
        # Parse each line, considering commas in captions
        parts = line.strip().split(',', 1)  # Split only at first comma
        if len(parts) == 2:
            image_name, caption = parts
            # Check if this image is one of our targets
            if image_name in target_images:
                image_captions[image_name].append(caption)

# Display each image with its captions
for image_name in target_images:
    # Load and display the image
    image_path = os.path.join(image_dir, image_name)
    try:
        img = Image.open(image_path)
        plt.figure(figsize=(8, 6))
        plt.imshow(img)
        plt.axis('off')

        # Construct caption text
        caption_text = f"Image: {image_name}\n"
        if image_captions[image_name]:
            for i, caption in enumerate(image_captions[image_name], 1):
                caption_text += f"Caption {i}: {caption}\n"
        else:
            caption_text += "No captions found."

        plt.title(caption_text, fontsize=10, wrap=True)
        plt.tight_layout()
        plt.show()

        # Also print the captions to console
        print(f"\nCaptions for {image_name}:")
        if image_captions[image_name]:
            for i, caption in enumerate(image_captions[image_name], 1):
                print(f"Caption {i}: {caption}")
        else:
            print("No captions found.")

    except FileNotFoundError:
        print(f"Image file not found: {image_path}")
    except Exception as e:
        print(f"Error displaying image {image_name}: {e}")

!pip install nltk
import nltk
nltk.download('punkt_tab')

import pandas as pd
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import matplotlib.pyplot as plt
import os
from PIL import Image

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# List of target images (in order)
target_images = [
    "3581818450_546c89ca38.jpg",
    "3208987435_780ae35ef0.jpg",
    "3139895886_5a6d495b13.jpg",
    "1354318519_2f9baed754.jpg",
    "3381788544_2c50e139dd.jpg",
    "756004341_1a816df714.jpg"
]

# The full predicted captions exactly as provided
predicted_captions = [
    "two dogs are running in the grass end end end end end end end end end end end end end end end end end end end end end end end end end end end end end end",
    "a young girl in a blue shirt and a black dog standing in a field of a white dog end end end end end end end end end end end end end end end end end end",
    "a dog is running through a grassy area end end end end end end end end end end end end end end end end end end end end end end end end end end end end end",
    "a man in a red shirt and a black shirt is smiling while a man in a red shirt looks at the camera end of a woman in a red dress end end end end end end",
    "three men in red shirts and white shirts are posing for a picture end of a picture end of a picture end end end end end end end end end end end end end end end end",
    "two smiling people posing for a picture end of a picture end end end end end end end end end end end end end end end end end end end end end end end end end end"
]

# Path to image directory
image_dir = "DL_Datasets/compressed_images"

# Load the actual captions from the file
caption_path = "DL_Datasets/captions.txt"
reference_captions = {img: [] for img in target_images}

# Open and read the file to get reference captions
with open(caption_path, 'r') as file:
    # Skip header line
    next(file)
    for line in file:
        parts = line.strip().split(',', 1)  # Split at first comma
        if len(parts) == 2:
            image_name, caption = parts
            if image_name in target_images:
                reference_captions[image_name].append(caption.strip())

# Calculate BLEU scores for each image
smoothie = SmoothingFunction().method1  # Smoothing for short sentences
results = []

for i, image in enumerate(target_images):
    # Tokenize predicted caption (use the full caption as provided)
    predicted_tokens = nltk.word_tokenize(predicted_captions[i].lower())

    # Tokenize reference captions for this image
    references_tokens = [nltk.word_tokenize(ref.lower()) for ref in reference_captions[image]]

    # If we have reference captions, calculate BLEU score
    if references_tokens:
        # Calculate BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores
        bleu1 = sentence_bleu(references_tokens, predicted_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)
        bleu2 = sentence_bleu(references_tokens, predicted_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)
        bleu3 = sentence_bleu(references_tokens, predicted_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)
        bleu4 = sentence_bleu(references_tokens, predicted_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)

        results.append({
            'image': image,
            'predicted': predicted_captions[i],
            'references': reference_captions[image],
            'BLEU-1': bleu1,
            'BLEU-2': bleu2,
            'BLEU-3': bleu3,
            'BLEU-4': bleu4
        })
    else:
        results.append({
            'image': image,
            'predicted': predicted_captions[i],
            'references': 'No reference captions found',
            'BLEU-1': 'N/A',
            'BLEU-2': 'N/A',
            'BLEU-3': 'N/A',
            'BLEU-4': 'N/A'
        })

# Display results with images
for result in results:
    image_name = result['image']
    image_path = os.path.join(image_dir, image_name)

    try:
        # Display the image
        img = Image.open(image_path)
        plt.figure(figsize=(10, 8))
        plt.imshow(img)
        plt.axis('off')

        # Create title with basic information
        plt.title(f"Image: {image_name}", fontsize=12)
        plt.tight_layout()
        plt.show()

        # Print detailed information
        print(f"\nImage: {image_name}")
        print(f"Predicted: {result['predicted']}")
        print("References:")

        if isinstance(result['references'], list):
            for i, ref in enumerate(result['references'], 1):
                print(f"  {i}. {ref}")
            print(f"BLEU-1 Score: {result['BLEU-1']:.4f}")
            print(f"BLEU-2 Score: {result['BLEU-2']:.4f}")
            print(f"BLEU-3 Score: {result['BLEU-3']:.4f}")
            print(f"BLEU-4 Score: {result['BLEU-4']:.4f}")
        else:
            print(f"  {result['references']}")

    except FileNotFoundError:
        print(f"Image file not found: {image_path}")
        print(f"BLEU scores for {image_name}:")
        if isinstance(result['BLEU-1'], float):
            print(f"BLEU-1: {result['BLEU-1']:.4f}")
            print(f"BLEU-2: {result['BLEU-2']:.4f}")
            print(f"BLEU-3: {result['BLEU-3']:.4f}")
            print(f"BLEU-4: {result['BLEU-4']:.4f}")
    except Exception as e:
        print(f"Error displaying image {image_name}: {e}")

# Calculate and display average BLEU scores
valid_results = [r for r in results if isinstance(r['BLEU-1'], float)]
if valid_results:
    avg_bleu1 = sum(r['BLEU-1'] for r in valid_results) / len(valid_results)
    avg_bleu2 = sum(r['BLEU-2'] for r in valid_results) / len(valid_results)
    avg_bleu3 = sum(r['BLEU-3'] for r in valid_results) / len(valid_results)
    avg_bleu4 = sum(r['BLEU-4'] for r in valid_results) / len(valid_results)

    print("\n===== AVERAGE BLEU SCORES =====")
    print(f"Average BLEU-1: {avg_bleu1:.4f}")
    print(f"Average BLEU-2: {avg_bleu2:.4f}")
    print(f"Average BLEU-3: {avg_bleu3:.4f}")
    print(f"Average BLEU-4: {avg_bleu4:.4f}")
else:
    print("\nNo valid BLEU scores to average.")

